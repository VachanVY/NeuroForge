{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| JAX: 0.4.31 | Equinox: 0.11.4 |\n"
     ]
    }
   ],
   "source": [
    "import typing as tp\n",
    "import time\n",
    "\n",
    "import equinox as eqx\n",
    "from equinox import (\n",
    "    nn,\n",
    "    Module\n",
    ")\n",
    "import jax\n",
    "from jax import (\n",
    "    numpy as jnp,\n",
    "    Array\n",
    ")\n",
    "import optax\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"| JAX: {jax.__version__} | Equinox: {eqx.__version__} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([60000, 784])\n",
      "y_train.shape = torch.Size([60000])\n",
      "X_val.shape = torch.Size([10000, 784])\n",
      "y_val.shape = torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "train_data = trainset.data.float()\n",
    "y_train = trainset.targets\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True)\n",
    "X_VAL = testset.data.float()\n",
    "y_val = testset.targets\n",
    "\n",
    "X_train = train_data.reshape((-1, 28*28))/255.\n",
    "X_val = X_VAL.reshape((-1, 28*28))/255.\n",
    "\n",
    "num_classes = torch.unique(y_train)\n",
    "\n",
    "print(f\"X_train.shape = {X_train.shape}\\ny_train.shape = {y_train.shape}\\nX_val.shape = {X_val.shape}\\ny_val.shape = {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y): # (m, ...), (m, ...)\n",
    "    \"\"\"returns X, y (but shuffled)\"\"\"\n",
    "    idx = torch.randperm(X.shape[0])\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def batch(X, y, batch_size):\n",
    "    \"\"\"returns X, y (batched form of inputs X and y)\"\"\"\n",
    "    num_features = X.shape[-1]\n",
    "    rem_idx = None\n",
    "    if len(X)%batch_size != 0:\n",
    "        rem_idx = -(len(X)%batch_size)\n",
    "    X, y = shuffle(X, y) # (m, 784), (m, 10)\n",
    "    X, y = X[:rem_idx], y[:rem_idx]\n",
    "    X, y = X[None].reshape((-1, batch_size, num_features)), y[None].reshape((-1, batch_size))\n",
    "    return X, y # (m//B, B, 784), (m//B, B, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    batch_size = 32\n",
    "    output_size = 10\n",
    "    input_size = 784\n",
    "\n",
    "    epochs:int = 30\n",
    "    cumbersome_model_lr:float = ...\n",
    "    distilled_model_lr:float = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CumbersomeModel(Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        key:Array, \n",
    "        input_size:int=784, \n",
    "        hidden_size:int=1200, \n",
    "        output_size:int=10,\n",
    "        dropout_rates:list[float]=[0.2, 0.5]\n",
    "    ):\n",
    "        k1, k2, k3, k4, k5 = jax.random.split(key, 5)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size, key=k1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rates[0], key=k4)\n",
    "\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size, key=k2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rates[1], key=k5)\n",
    "\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size, key=k3)\n",
    "    \n",
    "    def __call__(self, x:Array, training:bool=True): # (784,)\n",
    "        x = self.dropout1(self.linear1(x), inference=not training) # (1200,)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dropout2(self.linear2(x), inference=not training) # (1200,)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.linear3(x) # (10,)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DistilledModel(CumbersomeModel):\n",
    "    def __init__(self, key:Array, hidden_size:int=800, dropout_rates:list[float]=[0.0, 0.0]):\n",
    "        super().__init__(key, hidden_size=hidden_size, dropout_rates=dropout_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: Module, \n",
    "    x: Array, # (B, 784)\n",
    "    y: Array  # (B,)\n",
    "):\n",
    "    pred_y = jax.vmap(model)(x, training=False) # (B, 10)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1) # (B,)\n",
    "    return jnp.mean(y == pred_y)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def soft_target_loss(\n",
    "    target_logits:Array, # (B, 10)\n",
    "    student_logits:Array, # (B, 10)\n",
    "    temperature:float\n",
    "):\n",
    "    target_probs = jax.nn.softmax(target_logits / temperature)\n",
    "    student_log_probs = jax.nn.log_softmax(student_logits / temperature)\n",
    "    return -jnp.sum(target_probs * student_log_probs, axis=-1).mean(0)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def hard_target_loss(\n",
    "    targets:Array, # (B,)\n",
    "    logits:Array # (B, 10)\n",
    "):\n",
    "    return -jnp.sum(\n",
    "        jax.nn.one_hot(targets, config.output_size) * # target prob dist\n",
    "        jax.nn.log_softmax(logits), axis=-1   # student prob dist\n",
    "    ).mean(0)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def undistilled_model_loss(\n",
    "    model:CumbersomeModel|DistilledModel, \n",
    "    inputs:Array, # (B, 784)\n",
    "    targets:Array, # (B,)\n",
    "    training:bool=True\n",
    "):\n",
    "    predict = jax.vmap(model)(inputs, training=training)\n",
    "    return hard_target_loss(targets, predict)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def distilled_model_loss(\n",
    "    model:DistilledModel,\n",
    "    target_model:CumbersomeModel,\n",
    "    hard_targets:Array, # (B,)\n",
    "    inputs:Array, # (B, 784)\n",
    "    temperature:float,\n",
    "    w:float,\n",
    "    training:bool=True\n",
    "):\n",
    "    assert 0 <= w <= 1, \"w must be in [0, 1]\"\n",
    "    target_logits = jax.lax.stop_gradient(jax.vmap(target_model)(inputs, training=False))\n",
    "    student_logits = jax.vmap(model)(inputs, training=training)\n",
    "    return (\n",
    "        w* soft_target_loss(target_logits, student_logits, temperature=temperature) +\n",
    "        (1-w) * hard_target_loss(hard_targets, student_logits)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cumbersome_model(\n",
    "    model:CumbersomeModel, X_train:Array, y_train:Array, X_val:Array, y_val:Array\n",
    "):\n",
    "    X_train, y_train = batch(X_train, y_train, config.batch_size) # (m//B, B, 784), (m//B, B, 10)\n",
    "    X_val, y_val = batch(X_val, y_val, config.batch_size) # (m//B, B, 784), (m//B, B, 10)\n",
    "\n",
    "    optim = optax.adamw(learning_rate=config.cumbersome_model_lr, weight_decay=1e-4)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def update(\n",
    "        model:CumbersomeModel, \n",
    "        opt_state:optax.OptState, \n",
    "        inputs:Array, \n",
    "        targets:Array\n",
    "    ):\n",
    "        loss, grads = eqx.filter_value_and_grad(undistilled_model_loss)(model, inputs, targets)\n",
    "        updates, opt_state = optim.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "    \n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        #          (B, 784), (B,)\n",
    "        for (step, (inputs, targets)) in enumerate(zip(X_train, y_train)):\n",
    "            inputs, targets = inputs.numpy(), targets.numpy()\n",
    "            model, opt_state, loss = update(model, opt_state, inputs, targets); losses.append(loss)\n",
    "        \n",
    "        train_accuracy = compute_accuracy(model, X_train.reshape((-1, 784)), y_train.reshape(-1))\n",
    "        val_accuracy = compute_accuracy(model, X_val.reshape((-1, 784)), y_val.reshape(-1))\n",
    "\n",
    "        mean_train_loss = jnp.array(losses).mean()\n",
    "        mean_val_loss = undistilled_model_loss(model, X_val.reshape((-1, 784)), y_val.reshape(-1), training=False)\n",
    "        dt = time.time() - t0\n",
    "        print(\n",
    "            f\"Epoch: {epoch}/{config.epochs} | Train Loss: {mean_train_loss:.4f} | Validation Loss: {mean_val_loss:.4f} |\"\n",
    "            f\"Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {val_accuracy:.4f} |\"\n",
    "            f\"dt per epoch: {dt}s |\"\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilled_model_without_cumbersome_model(\n",
    "    model:DistilledModel, \n",
    "    X_train:Array, y_train:Array, X_val:Array, y_val:Array\n",
    "):\n",
    "    X_train, y_train = batch(X_train, y_train, config.batch_size) # (m//B, B, 784), (m//B, B, 10)\n",
    "    X_val, y_val = batch(X_val, y_val, config.batch_size) # (m//B, B, 784), (m//B, B, 10)\n",
    "\n",
    "    optim = optax.adamw(learning_rate=config.distilled_model_lr, weight_decay=0e-4)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def update(\n",
    "        model:DistilledModel, \n",
    "        opt_state:optax.OptState, \n",
    "        inputs:Array, \n",
    "        targets:Array\n",
    "    ):\n",
    "        loss, grads = eqx.filter_value_and_grad(undistilled_model_loss)(model, inputs, targets)\n",
    "        updates, opt_state = optim.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "    \n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        #          (B, 784), (B,)\n",
    "        for (step, (inputs, targets)) in enumerate(zip(X_train, y_train)):\n",
    "            inputs, targets = inputs.numpy(), targets.numpy()\n",
    "            model, opt_state, loss = update(model, opt_state, inputs, targets); losses.append(loss)\n",
    "        \n",
    "        train_accuracy = compute_accuracy(model, X_train.reshape((-1, 784)), y_train.reshape(-1))\n",
    "        val_accuracy = compute_accuracy(model, X_val.reshape((-1, 784)), y_val.reshape(-1))\n",
    "\n",
    "        mean_train_loss = jnp.array(losses).mean()\n",
    "        mean_val_loss = undistilled_model_loss(model, X_val.reshape((-1, 784)), y_val.reshape(-1), training=False)\n",
    "        dt = time.time() - t0\n",
    "        print(\n",
    "            f\"Epoch: {epoch}/{config.epochs} | Train Loss: {mean_train_loss:.4f} | Validation Loss: {mean_val_loss:.4f} |\"\n",
    "            f\"Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {val_accuracy:.4f} |\"\n",
    "            f\"dt per epoch: {dt}s |\"\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilled_model_with_cumbersome_model(\n",
    "    model:DistilledModel, cumbersom_model:CumbersomeModel,\n",
    "    X_train:Array, y_train:Array, X_val:Array, y_val:Array\n",
    "):\n",
    "    X_train, y_train = batch(X_train, y_train, config.batch_size) # (m//B, B, 784), (m//B, B, 10)\n",
    "    X_val, y_val = batch(X_val, y_val, config.batch_size) # (m//B, B, 784), (m//B, B, 10)\n",
    "\n",
    "    optim = optax.adamw(learning_rate=config.distilled_model_lr, weight_decay=0e-4)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def update(\n",
    "        model:DistilledModel, \n",
    "        opt_state:optax.OptState, \n",
    "        inputs:Array, \n",
    "        targets:Array\n",
    "    ):\n",
    "        loss, grads = eqx.filter_value_and_grad(distilled_model_loss)(model, cumbersom_model, inputs, targets, config.temperature, config.w)\n",
    "        updates, opt_state = optim.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "    \n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        #          (B, 784), (B,)\n",
    "        for (step, (inputs, targets)) in enumerate(zip(X_train, y_train)):\n",
    "            inputs, targets = inputs.numpy(), targets.numpy()\n",
    "            model, opt_state, loss = update(model, opt_state, inputs, targets); losses.append(loss)\n",
    "        \n",
    "        train_accuracy = compute_accuracy(model, X_train.reshape((-1, 784)), y_train.reshape(-1))\n",
    "        val_accuracy = compute_accuracy(model, X_val.reshape((-1, 784)), y_val.reshape(-1))\n",
    "\n",
    "        mean_train_loss = jnp.array(losses).mean()\n",
    "        mean_val_loss = distilled_model_loss(model, X_val.reshape((-1, 784)), y_val.reshape(-1), training=False)\n",
    "        dt = time.time() - t0\n",
    "        print(\n",
    "            f\"Epoch: {epoch}/{config.epochs} | Train Loss: {mean_train_loss:.4f} | Validation Loss: {mean_val_loss:.4f} |\"\n",
    "            f\"Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {val_accuracy:.4f} |\"\n",
    "            f\"dt per epoch: {dt}s |\"\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the models, plot the results and compare the results. This is not yet complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
