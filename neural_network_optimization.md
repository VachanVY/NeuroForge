# Neural Network Optimization
* <img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/d4e66c8d-ee41-403a-a7ce-add3a707b2e3" />
* <img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/9a192232-04cc-4467-8b51-52af23c558eb" />
---
* <img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/1c04b78d-52de-4bfc-a70d-fb0abfbd34a8" />
* <img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/fa934ea4-dfb4-4d91-afaf-f42ba7530db2" />
---
* <img width="1913" height="494" alt="image" src="https://github.com/user-attachments/assets/67e03ce6-6096-4396-ab29-81ddf496ca49" />
These up and down oscillations slow down gradient descent, preventing the use of large learning rates.
* <img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/6ed4bc57-a963-4305-92cb-ebfe251062aa" />
---
* Adaptive Learning Rate
 <img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/1ca8b59b-7b41-457d-bea9-53f080954a01" />
 <img width="1599" height="1000" alt="image" src="https://github.com/user-attachments/assets/b016c0c5-3020-4c09-b10c-92910bb4cc1a" />
 <details>
  <summary> Difference Between RMSProp and AdaGrad (ChatGpt) </summary>
  <img width="894" height="1024" alt="image" src="https://github.com/user-attachments/assets/1035d364-177d-4d93-a9ac-228d75ed63b4" />
 </details>
 
---

* Adam
  <img width="1914" height="998" alt="image" src="https://github.com/user-attachments/assets/8553b369-d9e4-4364-90d6-0f28076e52af" />

---


---
# Additional Notes on Neural Network Optimization
* <img width="871" height="439" alt="image" src="https://github.com/user-attachments/assets/340396ac-7f02-4fa4-ac69-ce1b76fa7f08" />

---
